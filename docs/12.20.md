用flink 将mysql表读取收打成jar包 部署yarn

flink cdc read mysql
> MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
                .hostname("cdh03")
                .port(3306)
                .databaseList("jianchao_ilya") // 设置捕获的数据库， 如果需要同步整个数据库，请将 tableList 设置为 ".*".
                .tableList("jianchao_ilya.user") // 设置捕获的表
                .username("root")
                .password("root")
                .startupOptions(StartupOptions.earliest())
                .deserializer(new JsonDebeziumDeserializationSchema()) // 将 SourceRecord 转换为 JSON 字符串
                .build();
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 设置 3s 的 checkpoint 间隔
        env.enableCheckpointing(3000);
        DataStreamSource<String> data = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), "MySQL Source");
        data.print();

sink to kafka
> // 配置 Kafka Sink
KafkaSink<String> sink = KafkaSink.<String>builder()
.setBootstrapServers("cdh01:9092")
.setRecordSerializer(KafkaRecordSerializationSchema.builder()
.setTopic("topic_dev1")
.setValueSerializationSchema(new SimpleStringSchema())
.build()
)
.setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
.build();
 data.sinkTo(sink);


部署上传yarn时报错
org.apache.kafka.common.KafkaException:Failed to construct kafka producer
解决方法
>方式一：
跟flink的类加载方式有关，即flink-conf.yml中的classloader.resolve-order参数，要将默认的
child-first改成parent-first，修改后就ok了
方式二：
方式一的方法是治标不治本，其实报这个错的原因，是个依赖的问题。
就是因为flink-connector-kafka.jar依赖冲突了。
如果集群flink/lib下已经有了flink-connector-kafka.jar，那就要自己任务中的pom里面就要将kafka的connector依赖provider一下。





